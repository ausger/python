# -*- coding: utf-8 -*-
import scrapy
import re
from crawlingdm.items import DmItem
from crawlingdm.helper.stringtools import StringTools
from scrapy.http.request import Request
from lxml import etree

# extends scrapy.Spider only to utilize its pipeline. Start_Url can be whatever, as it is crawled but will not be parsed
# The reason: Dm products are generated by React.js on the fly, so it's html source code contains only placeholder
# for the content. We have to saved the ajax response manually into the html file.
# This class is used to parse the html file by means of lxml library.


class DmReactContentParser(scrapy.Spider):
    # Scrapy is single-threaded, except the interactive shell and some tests, see source.
    # Scrapy does most of it's work synchronously. However, the handling of requests is done asynchronously.
    name = "dm"
    html_file = 'E:/python/crawlingdm/input/brand-belea-page-20.html'
    html_files = 'E:/python/crawlingdm/input/brand-belea-page/4-20.html'
    allowed_domains = ["www.dm.de"]

    def __init__(self, *a, **kw):
        super(DmReactContentParser, self).__init__(*a, **kw)
        self.start_urls = ['https://www.dm.de']
        self.generate_html_range()
        self.tree = etree.parse(DmReactContentParser.html_file, etree.HTMLParser())

    def generate_html_range(self):
        self.html_trees = []
        file_range = DmReactContentParser.html_files.split('/')
        tmp = file_range[-1]
        tmp2 = tmp.split('.')
        page_range = tmp2[0]
        if '-' in page_range:
            page_range = page_range.split('-')

    PRODUCT_DETAILS_LINK = "//div[@class='product-tile-image-container']/a/@href"

    NOT_AVAILABLE_PATH = "//div[@class='product-attributes-info-availability']/descendant::" \
                         "span[contains(concat(' ',normalize-space(@class),''), ' availability-inStock')]/text()"

    NAME_PATH = "//div[contains(concat(' ',normalize-space(@class),''), ' product-headline')]" \
                "/h1/span[@itemprop='name']/text()"
    BRAND_PATH = "//div[contains(concat(' ',normalize-space(@class),''), ' product-headline')]" \
                 "/h2/span[@itemprop='brand']/span[@itemprop='name']/text()"

    IMG_SRC = "//div[@id='product-image']/descendant::img[@itemprop='image']/@src"

    IMG_TITLE = "//div[@id='product-image']/descendant::img[@itemprop='image']/@alt"

    PRICE_PATH = "//div[@class='product-attributes-info-availability']/descendant::" \
                 "span[@itemprop='price']/@content"

    MISCELLANEOUS_PATH = "//div[@class='product-attributes-info-price']/text()"

    item_count = 0

    # start_urls = [amazon_cn_search_result_link + "%s" % n for n in product_ids]

    def parse(self, response):
        # print response.request.headers['User-Agent']
        # print response.request.headers.get('Referrer', None)
        items = self.tree.xpath(self.PRODUCT_DETAILS_LINK)
        self.item_count = len(items)
        for index, item in enumerate(items):
            details_url = self.start_urls[0] + item
            print('crawling %d. element: %s' % (index, details_url))
            request = Request(details_url, callback=self.parse_product_details, meta={'item_index': index})
            yield request

    def parse_product_details(self, response):
        availability = response.xpath(self.NOT_AVAILABLE_PATH).extract()
        product_name_holder = response.xpath(self.NAME_PATH).extract()
        if len(availability) > 0:
            dm_item = DmItem()
            # NAME_PATH, IMG_SRC, IMG_TITLE, IMG_GALLERY_SRC, PRICE_PATH, SHORT_DESCRIPTION_PATH, DESCRIPTION_PATH
            self.handle_product_name(response, dm_item)
            self.handle_image(response, dm_item)
            self.handle_price(response, dm_item)
            self.handle_miscellaneous(response, dm_item)
            yield dm_item
        else:
            print('product[%s] is out of stock' % product_name_holder[0])

    def handle_miscellaneous(self, response, world_sweets_item):
        product_miscellaneous = response.xpath(self.MISCELLANEOUS_PATH).extract()
        if len(product_miscellaneous) > 0:
            world_sweets_item['miscellaneous'] = product_miscellaneous[0]
            print('miscellaneous is %s ' % world_sweets_item['miscellaneous'])

    def handle_price(self, response, world_sweets_item):
        product_price = response.xpath(self.PRICE_PATH).extract()
        tmp = product_price[0]
        print("price was %s" % product_price[0])
        tmp = re.sub('((\s)*€(\s)*)', '', tmp)
        world_sweets_item['price'] = tmp.strip()
        print("now price is %s" % world_sweets_item['price'])

    def handle_image(self, response, world_sweets_item):
        world_sweets_item['image_urls'] = response.xpath(self.IMG_SRC).extract()[0]
        # image_name = '/' + self.generate_image_name(response.xpath(self.IMG_SRC).extract()[0])
        image_name = self.generate_image_name(response.xpath(self.IMG_SRC).extract()[0])
        world_sweets_item['image'] = image_name
        world_sweets_item['image_label'] = response.xpath(self.IMG_TITLE).extract()[0]

        print('image src is %s' % world_sweets_item['image'])
        print('image label is %s' % world_sweets_item['image_label'])

    def handle_product_name(self, response, world_sweets_item):
        product_name_holder = response.xpath(self.NAME_PATH).extract()
        if len(product_name_holder) > 0:
            name_tmp = product_name_holder[0]
            url_info = self.build_url_info(name_tmp)
            world_sweets_item['sku'] = url_info
            world_sweets_item['name'] = name_tmp

    def build_url_info(self, name_tmp):
        # replace comma & space with -
        # str.replace() does not recognize regular expressions, to perform a substitution using
        # regular expressions use re.sub().
        # print("original product title: %s" % name_tmp)
        name_tmp = name_tmp.strip()
        tmp = re.sub('((\s)*,(\s)*)', '-', name_tmp)
        tmp = re.sub('((\s)+)', '-', tmp)
        tmp = tmp.replace("/", "-")
        tmp = tmp.replace(".", "-")
        tmp = tmp.replace("+", "-")
        tmp = re.sub('((-)+)', '-', tmp)
        # replace german umlaut
        tmp = tmp.replace('ä', 'a')
        tmp = tmp.replace('Ä', 'a')
        tmp = tmp.replace('ö', 'o')
        tmp = tmp.replace('Ö', 'o')
        tmp = tmp.replace('ü', 'u')
        tmp = tmp.replace('Ü', 'u')
        tmp = tmp.replace('ß', 'ss')
        # print('dashed title %s ' % tmp)
        return tmp.lower()

    def generate_image_name(self, product_image):
        # product_images is like http://www.liwus.de/media/catalog/product/cache/1/
        # image/9df78eab33525d08d6e5fb8d27136e95/N/U/NUK_10176092.jpg
        prod_image_tmp = product_image.split('/')
        # image_name is the last part in the product_images[0], i.e. NUK_10176092.jpg
        # image_name = prod_image_tmp[-3:len(prod_image_tmp)]
        # image_name = '/'.join(image_name)
        image_name = prod_image_tmp[-1]
        return image_name.lower()

    def generate_sku(self, dashed_string, a_normal_sentence):
        result = ''
        truncated = re.sub(r'\s+', '', a_normal_sentence.lower())
        first_12_chars = truncated[:12]
        print('first 12 characters %s' % first_12_chars)
        splitted = dashed_string.split('-')
        for idx, e in enumerate(splitted):
            # print((idx, e))
            if e in first_12_chars:
                # print('found ' + e + ' in ' + first_12_chars)
                if idx == 0:
                    result = result.join(e)
                    print('result is ' + result)
                else:
                    result = '-'.join([result, e])
                    print('result is ' + result)
            else:
                common = StringTools.longest_common_substring(e, first_12_chars)
                # print(common)
                result = '-'.join([result, common])
                print('result is ' + result)
                return result
